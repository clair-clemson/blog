---
layout:     post
title:      Notes on High-Dimensional Probabilities Chapter 1-5
date:		"2023-07-21 09:53:09"
author:     Yifan Kang
visible:    false
---

# Preliminaries

## Jensen's inequality
> For any random variable $X$ and a *convex* function $\varphi :\mathbb{R}\to\mathbb{R}$, we have
> $$\varphi(\mathbb{E}X)\leq\mathbb{E}\varphi (X)$$
[Proof](https://en.wikipedia.org/wiki/Jensen%27s_inequality#Proof_3_(general_inequality_in_a_probabilistic_setting))
## Integral Identity
### Non-negative Random Variable
> For a *non-negative* random variable $X$
> $$\mathbb{E}X=\int_0^\infty\mathbb{P}\{X>t\}dt$$
Proof: via expectation of identity $x=\int^x_0 1dt=\int_0^\infty\mathbf{1}_{(t<x)}dt$
### Generalization
> For *any* random variable $X$ (not neccessarily non-negative)
> $$\mathbb{E}X=\int_0^\infty\mathbb{P}\{X>t\}dt-\int^0_{-\infty}\mathbb{P}\{X<t\}dt$$
Proof: condition on non-negative / negative

## Markov's Inequality
> For any *non-negative* random variable $X$ and $t>0$
> $$\mathbb{P}\{X\geq t\}\leq\frac{\mathbb{E}X}{t}.$$
Proof: via identity $x=x\mathbf{1}_{\{x\geq t\}}+x\mathbf{1}_{\{x< t\}}$

## Chebyshev's Inequality
> Let $X$ be a random variable with mean $\mu$ and variance $\sigma^2$. Then for any $t>0$
> $$\mathbb{P}\{|X-\mu|\geq t\}\leq \frac{\sigma^2}{t^2}.$$
Proof: Squaring both sides & Markov

## Lindeberg-L$\acute{e}$vy Central Limit Theorem
> Let $X_1,X_2,...$ be i.i.d. random variables with mean $\mu$ and variance $\sigma^2$. Consider the sum 
> $$S_N = X_1+\cdots +X_N$$
> and the normalized sum
> $$Z_n:=\frac{S_N-\mathbb{E}S_N}{\sqrt{\text{Var}(S_N)}}=\frac{1}{\sigma\sqrt{N}}\sum_{i=1}^N(X_i-\mu).$$
> As $N\to \infty,$
> $$Z_N\to N(0,1)\qquad in~distribution.$$


# Concentration of Sums

## Tails of Normal Distribution

> $g\sim N(0,1)$, for all $t>0$
> $$\left(\frac{1}{t}-\frac{1}{t^3}\right) \cdot \frac{1}{\sqrt{2 \pi}} e^{-t^2 / 2} \leq \mathbb{P}\{g \geq t\} \leq \frac{1}{t} \cdot \frac{1}{\sqrt{2 \pi}} e^{-t^2 / 2}.$$
> For $t\geq 1$
> $$\mathbb{P}\{g\geq t\}\leq\frac{1}{\sqrt{2\pi}}\int_t^\infty e^{x^2/2}dx.$$
Proof: Change variables $x=y+t$

## Sterling's Approximation

> $$\ln (n !)=n \ln n-n+O(\ln n)$$
> or more precisely
> $$n ! \sim \sqrt{2 \pi n}\left(\frac{n}{e}\right)^n$$

[Reference](https://en.wikipedia.org/wiki/Stirling%27s_approximation)

## Hoeffding's Inequality

> Let $X_1,...,X_N$ be independent symmetric Bernouli random variables, and $a=(a_1,...,a_N)\in\mathbb{R}^N$. For any $t>0$,
> $$\mathbb{P}\{\sum_{i=1}^Na_iX_i\geq t\}\leq\exp(-\frac{t^2}{2\|a\|_2^2}).$$

Proof: P16, Markov's inequality with multiplier $\lambda$, use cosh bound MGF.

### Two-sided
> $$\mathbb{P}\{|\sum_{i=1}^Na_iX_i|\geq t\}\leq2\exp(-\frac{t^2}{2\|a\|_2^2}).$$

### General Bounded Random Variables
> For $X_i\in[m_i,M_i]$
> $$\mathbb{P}\left\{\sum_{i=1}^N\left(X_i-\mathbb{E} X_i\right) \geq t\right\} \leq \exp \left(-\frac{2 t^2}{\sum_{i=1}^N\left(M_i-m_i\right)^2}\right)$$

## Chernoff's Inequality

> $X_i\sim$ Bernouli$(p)$. $S_N=\sum_{i=1}^NX_i,~\mu=\mathbb{E}S_N$. For $t>\mu$
> $$\mathbb{P}\{S_N\geq t\}\leq e^{-\mu}(\frac{e\mu}{t})^t$$

Proof: P20, like Hoeffding, with identity $1+x\leq e^x$ 

## Sub-Gaussian Distribution

### Properties
> For a random variable $X$, the following are equivalent with $K_i>0$:
> #### Tails of $X$
> $$\mathbb{P}\{|X|\geq t\}\leq2\exp(-t^2/K_1^2)\qquad\forall t\geq0$$
> #### Moments of $X$
> $$\|X\|_{L^P}=(\mathbb{E}|X|^p)^{1/p}\leq K_2\sqrt{p}\qquad\forall p\geq1$$
> #### MGF of $X^2$
> $$\mathbb{E}\exp(\lambda^2X^2)\leq\exp(K_3^2\lambda^2)\qquad\forall\lambda\quad s.t.\quad|\lambda|\leq\frac{1}{K_3}$$
> #### MGF of $X^2$ is bounded
> $$\mathbb{E}\exp(X^2/K_4^2)\leq2$$
> #### If $\mathbb{E}X=0$, the MGF of X
> $$\mathbb{E}\exp(\lambda X)\leq\exp(K^2_5\lambda^2)\qquad\forall\lambda\in\mathbb{R}$$

Proof: P25-27

### Definition
> The *sub-gaussian norm* of $X$, $\|X\|_{ \psi_2}$, is defined to be the smallest $K_4$
> $$\|X\|_{\psi_2}=\inf\{t>0:\mathbb{E}\exp(X^2/t^2)\leq2\}$$

## Examples of Sub-Gaussian Distributions
### Gaussian
> $$X\sim N(0,\sigma^2)\qquad\|X\|_{\psi_2}\leq C\sigma$$
### Bernoulli
> $$\|X\|_{\psi_2}=\frac{1}{\sqrt{\ln 2}}$$
### Bounded
> $$\|X\|_{\psi_2}\leq C\|X\|_\infty$$

## General Hoeffding's Inequality
> Let $X_1,...,X_N$ be indenpendent, mean-zero, sub-gaussian random variables, for $t\geq9$,
> $$\mathbb{P\{|\sum^N_{i=1}X_i|\geq t\}\leq2\exp(-\frac{ct^2}{\sum_{i=1}^N\|X_i\|^2_{\psi_2}})}$$

### Khintchine's Inequality
> With unit variance and let $a\in\mathbb{R}^N$, for every $p\in[2,\infty)$
> $$(\sum_{i=1}^N a_i^2)^{1/2}\leq \|\sum_{i=1}^Na_iX_i\|_{L^P}\leq CK\sqrt{p}(\sum_{i=1}^Na_i^2)^{1/2}$$

## Sub-Exponential Distributions

### Properties
> For a random variable $X$, the following are equivalent with $K_i>0$:
> #### Tails of $X$
> $$\mathbb{P}\{|X|\geq t\}\leq2\exp(-t/K_1)\qquad\forall t\geq0$$
> #### Moments of $X$
> $$\|X\|_{L^P}=(\mathbb{E}|X|^p)^{1/p}\leq K_2p\qquad\forall p\geq1$$
> #### MGF of $X^2$
> $$\mathbb{E}\exp(\lambda|X|)\leq\exp(K_3\lambda)\qquad\forall\lambda\quad s.t.\quad0\leq\lambda\leq\frac{1}{K_3}$$
> #### MGF of $X^2$ is bounded
> $$\mathbb{E}\exp(|X|/K_4)\leq2$$
> #### If $\mathbb{E}X=0$, the MGF of X
> $$\mathbb{E}\exp(\lambda X)\leq\exp(K^2_5\lambda^2)\qquad\forall\lambda\quad s.t.\quad|\lambda|\leq\frac{1}{K_5}$$
### Definition
> The *sub-Eexponential norm* $\|X\|_{ \psi_1}$
> $$\|X\|_{\psi_2}=\inf\{t>0:\mathbb{E}\exp(|X|/t)\leq2\}$$

## Berstein's Inequality

> Let $X_1,...X_N$ be indenpendent, mean-zero, sub-exponential random variables. For $t\geq0$
> $$\mathbb{P}\{|\sum_{i=1}^NX_i|\geq t\}\leq2\exp[-c\min(\frac{t^2}{\sum_{i=1}^N\|X_i\|^2_{\psi_1}},\frac{t}{\max_i\|X_i\|_{\psi_1}})]$$

### For bounded Distributions
> Let $X_1,...X_N$ be indenpendent, mean-zero, s.t. $|X_i|\leq K$. For $t\geq0$
> $$\mathbb{P}\{|\sum_{i=1}^NX_i|\geq t\}\leq2\exp(-\frac{t^2/2}{\sigma^2+Kt/3})$$

# References
 - Vershynin, Roman. High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge University Press, 2018.